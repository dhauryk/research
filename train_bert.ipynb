{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDA посмотрел тут:\n",
    "\n",
    "https://medium.com/@ravitee/santander-product-recommendation-ee4122d15072\n",
    "\n",
    "https://medium.com/@samarthjoelram/santander-recommendation-system-cab6b40596b5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На ресерч, подбор параметров и тд времени мало, потому выбираю по интуиции. Только обучение берта на 3 эпохах занимат 15 часов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM, BertConfig   # Выбрал берт, тк раньше работал с ним, хоть и для других задач\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/dhauryk/!DUNAI/train_ver2.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "#df = df[-100000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нахожу series с продуктами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_columns = [col for col in df.columns if col.startswith(\"ind_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для создания последовательности с продуктами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sequence(row):\n",
    "    res_convert = \" \".join([col for col in product_columns if row[col] == 1])\n",
    "    \n",
    "    return res_convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тестовом наборе данных нету реальных меток. Для рассчета MAP7 отделяем тестовый набор из данных для обучения. Поскольку тестовый набор данных содержит будущий период, то сортируем свой тест и по дате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    df = df.sort_values([\"ncodpers\", \"fecha_dato\"])  # Сортируем по идентификатору клиента (ncodpers) и дате\n",
    "    df[\"product_sequence\"] = df.apply(convert_to_sequence, axis=1)  # Создаем последовательности продуктов для каждого клиента\n",
    "    unique_sequences = df.groupby(\"ncodpers\")[\"product_sequence\"].apply(lambda x: \" \".join(x)).reset_index()  # Удаляем дублирующиеся последовательности для каждого клиента\n",
    "    \n",
    "    return unique_sequences\n",
    "\n",
    "data = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока не перемешиваем, чтобы не потерять сортировку по датам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, temp_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.3, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем токинайзер и токенизируем данные для train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(list(train_data['product_sequence']), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_encodings['input_ids']))\n",
    "train_dataset = train_dataset.shuffle(1000).batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_encodings = tokenizer(list(val_data['product_sequence']), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_encodings['input_ids']))\n",
    "val_dataset = val_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_encodings = tokenizer(list(test_data['product_sequence']), padding=True, truncation=True, return_tensors=\"tf\")\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_encodings['input_ids']))\n",
    "test_dataset = test_dataset.batch(8).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгружаем модель и задаем конфиги. Конфиги меньше дефолтных ради экономии ресурсов и времени на обучении"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_hidden_layers=4, num_attention_heads=4, hidden_size=256)\n",
    "model = TFBertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задаем оптимизатор, лосс. Компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)  # 2 и более меток классов, потому SparseCategoricalCrossentropy\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение и пишем историю для просмотра, чтобы исключить подгон под валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=3)   # Берту обычно хватает 2-4 эпохи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим лосс на обучении и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подаем в модель тестовые данные, сортируем и берем топ 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 7\n",
    "predictions = model.predict(test_encodings['input_ids']).logits\n",
    "predicted_tokens = np.argsort(-predictions, axis=-1)[:, :, :k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчета MAP7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map7(preds, actuals, k=7):\n",
    "    avg_precisions = []  # Сюда собираем средние точности каждой последовательности\n",
    "    for pred, actual in zip(preds, actuals):\n",
    "        match = 0  # Счетчик совпадений\n",
    "        sum_precisions = 0  # Сумма точности в каждом ранге\n",
    "        for i, p in enumerate(pred[:k]):  # Берем топ 7\n",
    "            if p in actual:  # Проверяем есть ли прогноз в актуальном списке\n",
    "                match += 1\n",
    "                sum_precisions += match / (i + 1)  # Точность вычисления на i-м ранге\n",
    "        avg_precisions.append(sum_precisions / min(k, len(actual)))  # Считаем среднюю точность\n",
    "\n",
    "    return np.mean(avg_precisions)  # Возвращаем среднее значения всех средних значений точности\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразовываем реальные последовательности продуктов в идентификаторы токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_tokens = [tokenizer.encode(seq, add_special_tokens=False) for seq in test_data['product_sequence']]\n",
    "map7_score = map7(predicted_tokens, actual_tokens, k)\n",
    "print(f\"MAP@7 Score: {map7_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем модель и токинайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"bert4rec\")\n",
    "tokenizer.save_pretrained(\"bert4rec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
