{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_feature = StandardScaler()\n",
    "\n",
    "df = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "\n",
    "df = df[1:-1]\n",
    "\n",
    "for col in df.columns:\n",
    "    if \"Close\" in col or \"Low\" in col or \"High\" in col:\n",
    "        if \"Change\" not in col:\n",
    "            for idx in df[col].index:\n",
    "                if idx < 748:\n",
    "                    df.loc[idx, f\"Next_{col}\"] = df.loc[idx+1, col]\n",
    "\n",
    "df = df[1:-1]\n",
    "\n",
    "X = df\n",
    "for col in X.columns:\n",
    "    if \"Target\" in col or \"Spread\" in col or \"Change\" in col or \"Open\" in col or \"Date\" in col or \"Next\" in col:\n",
    "        if \"Open_Interest_All\" != col:\n",
    "            X = X.drop([col], axis=1)\n",
    "\n",
    "    X_scaler = scaler_feature.fit_transform(X)\n",
    "\n",
    "y = df.Target_Open_Interest_All\n",
    "\n",
    "LEN_INPUT = len(X.columns)\n",
    "LEN_OUTPUT = 1 #len(y.columns)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "#os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "# from keras_tuner_cv.outer_cv import OuterCV\n",
    "# from keras_tuner_cv.inner_cv import inner_cv\n",
    "# from keras_tuner_cv.utils import pd_inner_cv_get_result\n",
    "# from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import Objective\n",
    "from keras import regularizers\n",
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_regression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_feature = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "\n",
    "\n",
    "def reg_wrapper(type, value):\n",
    "    if type == 'l2':\n",
    "        return regularizers.l2(value)\n",
    "    if type == 'l1':\n",
    "        return regularizers.l1(value)\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, start_from_epoch=0, restore_best_weights=True)\n",
    "\n",
    "X_scaler = scaler_feature.fit_transform(X)\n",
    "#y_scaler = scaler_target.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    #num_layers = hp.Int('num_layers', min_value=1, max_value=2, step=1)\n",
    "    reg = reg_wrapper(hp.Choice('type', ['l2']), hp.Choice('reg_value', [0.00001]))\n",
    "    input_layer = keras.Input(shape=(LEN_INPUT,), )\n",
    "    model = keras.Sequential()\n",
    "    model.add(input_layer)\n",
    "    model.add(layers.Dense(units=hp.Int(f'units_1', min_value=2048, max_value=10240, step=1024), kernel_initializer='glorot_uniform', activation='relu', activity_regularizer=reg))\n",
    "    # for i in range(num_layers):\n",
    "    #     if i == 0:\n",
    "    #         model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=10240, step=256), kernel_initializer='glorot_uniform', activation='relu', activity_regularizer=reg))\n",
    "    #     if i == 1:\n",
    "    #         model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=2048, step=32), kernel_initializer='glorot_uniform', activation='relu', activity_regularizer=reg))\n",
    "        # if i == 2:\n",
    "        #     model.add(layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=2048, step=128), kernel_initializer='glorot_uniform', activation='relu', activity_regularizer=reg))\n",
    "\n",
    "    model.add(layers.Dense(LEN_OUTPUT, activation='linear'))\n",
    "\n",
    "    #hp_loss = hp.Choice(\"loss\", values=['mae', 'mse'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.00001])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss='mae', metrics=['mean_absolute_error'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "build_model(kt.HyperParameters())\n",
    "\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel = build_model,\n",
    "    objective=Objective(\"val_mean_absolute_error\", \n",
    "                        direction=\"min\"),\n",
    "    directory=\"Keras_tuner_dir\",\n",
    "    project_name=\"Keras_tuner_Demo\", \n",
    "    overwrite=True,\n",
    "    seed=42,\n",
    "    max_trials=2500,\n",
    "    \n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=1000000,\n",
    "    callbacks=[callback],\n",
    "    verbose=True,\n",
    "    batch_size=len(y)\n",
    "    \n",
    ")\n",
    "\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "print(best_hps)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3697 - 9728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)  # Gets the top 2 models\n",
    "best_model = models[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(\"my_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/fit/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, start_from_epoch=0, restore_best_weights=True)\n",
    "\n",
    "input_layer = keras.Input(shape=(LEN_INPUT,), )\n",
    "model = keras.Sequential()\n",
    "model.add(input_layer)\n",
    "model.add(layers.Dense(9728, kernel_initializer='glorot_uniform', activation='relu', kernel_regularizer=keras.regularizers.L2(l2=0.00001)))\n",
    "model.add(layers.Dense(LEN_OUTPUT, activation='linear'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                  loss='mae', metrics=['mean_absolute_error'])\n",
    "\n",
    "model.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=100000, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[tensorboard_callback, callback],\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner import Objective\n",
    "from keras import regularizers\n",
    "import keras_tuner as kt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import model_selection\n",
    "from sklearn.datasets import make_regression\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_feature = StandardScaler()\n",
    "scaler_target = StandardScaler()\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, start_from_epoch=0, restore_best_weights=True)\n",
    "\n",
    "df = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "df = df.drop([\"Date\"], axis=1)\n",
    "df = df[1:-1]\n",
    "df = df.sample(frac = 1, random_state=42)\n",
    "feature_col_name = []\n",
    "LEN_INPUT = 0\n",
    "LEN_OUTPUT = 0\n",
    "for col in df.columns:\n",
    "    if \"Target\" not in col or \"Spread\" in col:\n",
    "        LEN_INPUT += 1\n",
    "    if \"Target\" in col or \"Spread\" in col:\n",
    "        feature_col_name.append(col)\n",
    "        LEN_OUTPUT += 1\n",
    "\n",
    "predicted_df = pd.DataFrame(columns=feature_col_name)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(df), 10):\n",
    "    input_layer = keras.Input(shape=(LEN_INPUT,), )\n",
    "    model = keras.Sequential()\n",
    "    model.add(input_layer)\n",
    "    model.add(layers.Dense(9728, kernel_initializer='glorot_uniform', activation='relu', kernel_regularizer=keras.regularizers.L2(l2=0.00001)))\n",
    "    model.add(layers.Dense(LEN_OUTPUT, activation='linear'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                    loss='mae', metrics=['mean_absolute_error'])\n",
    "\n",
    "    print(i, \"!!!!!!!!!!!\")\n",
    "    test_df = df[i:i+10]\n",
    "    temp_df1 = df[:i]\n",
    "    temp_df2 = df[i+10:]\n",
    "    frames = [temp_df1, temp_df2]\n",
    "    temp_df = pd.concat(frames)\n",
    "\n",
    "    X = temp_df\n",
    "    for col in X.columns:\n",
    "        if \"Target\" in col:\n",
    "            X = X.drop([col], axis=1)\n",
    "\n",
    "    y = temp_df\n",
    "    for col in y.columns:\n",
    "        if \"Target\" not in col:\n",
    "            y = y.drop([col], axis=1)\n",
    "\n",
    "    for col in test_df.columns:\n",
    "        if \"Target\" in col:\n",
    "            test_df = test_df.drop([col], axis=1)\n",
    "\n",
    "    test_data = np.array(test_df)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    \n",
    "    X_scaler = scaler_feature.fit_transform(X)\n",
    "    test_data_scaler = scaler_feature.transform(test_data)\n",
    "    y_scaler = scaler_target.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaler, y_scaler, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "    model.fit(x=X_train, \n",
    "            y=y_train, \n",
    "            epochs=100000, \n",
    "            validation_data=(X_test, y_test), \n",
    "            callbacks=[callback])\n",
    "    \n",
    "    predicted_t = model.predict(test_data_scaler)\n",
    "\n",
    "    predicted = pd.DataFrame(scaler_target.inverse_transform(predicted_t), columns=feature_col_name)\n",
    "    predicted.index = list(test_df.index)\n",
    "    frames = [predicted, predicted_df]\n",
    "    predicted_df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.to_csv('gen_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.index = list(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[i:i+10]\n",
    "for col in test_df.columns:\n",
    "    if \"Target\" not in col:\n",
    "        test_df = test_df.drop([col], axis=1)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "df = df.drop([\"Date\"], axis=1)\n",
    "df = df[1:-1]\n",
    "X = df\n",
    "for col in X.columns:\n",
    "    if \"Target\" in col or \"Spread\" in col or \"Change\" in col or \"Open\" in col:\n",
    "        if \"Open_Interest_All\" != col:\n",
    "            X = X.drop([col], axis=1)\n",
    "\n",
    "y = df.Target_Open_Interest_All\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"gen_df3.csv\", index_col=0)\n",
    "df = df.sort_index()\n",
    "df = df[:-1]\n",
    "\n",
    "X = df\n",
    "for col in X.columns:\n",
    "    if \"Next\" in col or (\"Target\" not in col and \"Close\" not in col and \"Low\" not in col and \"High\" not in col):\n",
    "        X = X.drop([col], axis=1)\n",
    "\n",
    "y = df.Next_Close_0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_feature = StandardScaler()\n",
    "#scaler_target = StandardScaler()\n",
    "\n",
    "X_scaler = scaler_feature.fit_transform(X)\n",
    "#y_scaler = scaler_target.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaler, y, test_size=0.05, shuffle=True, random_state=42)\n",
    "\n",
    "nn = MLPRegressor(verbose=False)\n",
    "\n",
    "hyperparameters = dict(hidden_layer_sizes=[200, 1000, 3000, 5000, 10000], solver=['adam'],\n",
    "                     alpha=[0.001, 0.0001, 0.00001], tol=[0.00001],\n",
    "                     learning_rate=['adaptive', 'constant'], learning_rate_init=[0.01, 0.001, 0.0001, 0.00001],\n",
    "                     max_iter=[500, 1000, 3000, 5000], early_stopping=[True],\n",
    "                     n_iter_no_change=[10])\n",
    "\n",
    "grid_search = GridSearchCV(nn, hyperparameters, n_jobs=-1, cv=5, verbose=2, error_score='raise', scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "\n",
    "\n",
    "hyperparameters_tuning = grid_search.fit(X_train, y_train)\n",
    "print('Best Parameters = {}'.format(hyperparameters_tuning.best_params_))\n",
    "\n",
    "tuned_model = hyperparameters_tuning.best_estimator_\n",
    "\n",
    "print(tuned_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Parameters = {}'.format(hyperparameters_tuning.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = pd.DataFrame(tuned_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "testScore = mean_absolute_error(tuned_model.predict(X_test), y_test)\n",
    "testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainScore = mean_absolute_error(tuned_model.predict(X_train), y_train)\n",
    "trainScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "res = tuned_model.predict(X_test)\n",
    "plt.plot(res)\n",
    "#plt.plot(trainPredictPlot)\n",
    "plt.plot(y_test.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_feature = StandardScaler()\n",
    "\n",
    "df = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "#df = df.drop([\"Date\"], axis=1)\n",
    "df = df[1:-1]\n",
    "gen_df = df.copy()\n",
    "for col in gen_df.columns:\n",
    "    if \"Target\" in col or \"Spread\" in col or \"Change\" in col or \"Open\" in col:\n",
    "        if \"Open_Interest_All\" != col:\n",
    "            gen_df = gen_df.drop([col], axis=1)\n",
    "\n",
    "X = df\n",
    "for col in X.columns:\n",
    "    if \"Target\" in col or \"Spread\" in col or \"Change\" in col or \"Open\" in col or \"Date\" in col:\n",
    "        if \"Open_Interest_All\" != col:\n",
    "            X = X.drop([col], axis=1)\n",
    "\n",
    "for col in df.columns:\n",
    "    if \"Target\" in col and \"Spread\" not in col and \"Change\" not in col:\n",
    "        for idx in range(X.index.min(), ((X.index.max()//50+1)*50), 50):\n",
    "            print(col, idx)\n",
    "            test_df = X[idx:idx+50]\n",
    "            temp_df1 = X[:idx]\n",
    "            temp_df2 = X[idx+50:]\n",
    "            frames = [temp_df1, temp_df2]\n",
    "            temp_df = pd.concat(frames)\n",
    "            X_scaler = scaler_feature.fit_transform(temp_df)\n",
    "\n",
    "            #test_df = df[col][idx:idx+10]\n",
    "            temp_df1 = df[col][:idx]\n",
    "            temp_df2 = df[col][idx+50:]\n",
    "            frames = [temp_df1, temp_df2]\n",
    "            y = pd.concat(frames)\n",
    "\n",
    "            X_train, _, y_train, _ = train_test_split(X_scaler, y, test_size=0.001, shuffle=True, random_state=42)\n",
    "            nn = MLPRegressor(alpha=1e-05, early_stopping=False, hidden_layer_sizes=5000, learning_rate='adaptive', learning_rate_init=0.001, max_iter=10000, n_iter_no_change=10, solver='adam', tol=1e-05, verbose=False)\n",
    "            nn.fit(X_train, y_train)\n",
    "            pred = nn.predict(test_df)\n",
    "            for i in range(len(pred)):\n",
    "                gen_df.loc[idx+i, col] = pred[i]\n",
    "\n",
    "            gen_df.to_csv('gen_df2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Это запустить\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "keras.utils.set_random_seed(42)\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "\n",
    "feature_col_name = []\n",
    "for col in dataframe.columns:\n",
    "\tif \"Target\" not in col and \"Spread\" not in col and \"Change\" not in col and \"Open\" not in col and \"High\" not in col and \"Low\" not in col and \"Close\" not in col and \"Date\" not in col or col == \"Open_Interest_All\":\n",
    "\t\tfeature_col_name.append(col)\n",
    "#predicted_df = pd.DataFrame(columns=feature_col_name, index=list(dataframe.index)[99:])\n",
    "predicted_df = pd.read_csv('gen_df2.csv', index_col=0)\n",
    "print(feature_col_name)\n",
    "for col in dataframe.columns:\n",
    "\tif \"Target\" not in col and \"Spread\" not in col and \"Change\" not in col and \"Open\" not in col and \"High\" not in col and \"Low\" not in col and \"Close\" not in col and \"Date\" not in col or col == \"Open_Interest_All\":\n",
    "\t\t\n",
    "\t\tfor i in range(0, len(dataframe)-100-1):\n",
    "\t\t\tif str(predicted_df.loc[100+i-1][col]) != \"nan\":\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tprint(col)\n",
    "\t\t\tdataset = dataframe.Open_Interest_All[:i+100]\n",
    "\t\t\t#print(i)\n",
    "\t\t\tdataset = dataset.values\n",
    "\t\t\tdataset = dataset.astype('float32')\n",
    "\n",
    "\t\t\t# normalize the dataset\n",
    "\t\t\tscaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\t\t\tdataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "\t\t\t# split into train and test sets\n",
    "\t\t\ttrain_size = int(len(dataset) - 8)\n",
    "\t\t\ttest_size = len(dataset) - train_size\n",
    "\t\t\ttrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\t\t\t# reshape into X=t and Y=t+1\n",
    "\t\t\tlook_back = 7\n",
    "\t\t\ttrainX, trainY = create_dataset(train, look_back)\n",
    "\t\t\ttestX, testY = create_dataset(test, look_back)\n",
    "\t\t\t# reshape input to be [samples, time steps, features]\n",
    "\t\t\t#print(scaler.inverse_transform(testX))\n",
    "\t\t\t#print(scaler.inverse_transform([testY]))\n",
    "\t\t\ttrainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "\t\t\ttestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\t\t\t# create and fit the LSTM network\n",
    "\t\t\tmodel = Sequential()\n",
    "\t\t\tmodel.add(LSTM(4, input_shape=(look_back, 1)))\n",
    "\t\t\tmodel.add(Dense(1))\n",
    "\t\t\tmodel.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\t\t\tmodel.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0)\n",
    "\t\t\t# make predictions\n",
    "\t\t\ttrainPredict = model.predict(trainX)\n",
    "\t\t\ttestPredict = model.predict(testX)\n",
    "\t\t\t# invert predictions\n",
    "\t\t\ttrainPredict = scaler.inverse_transform(trainPredict)\n",
    "\t\t\ttrainY = scaler.inverse_transform([trainY])\n",
    "\t\t\ttestPredict = scaler.inverse_transform(testPredict)[0][0]\n",
    "\t\t\t#print(testPredict)\n",
    "\t\t\tpredicted_df.loc[100+i-1][col] = testPredict\n",
    "\t\t\tpredicted_df.to_csv('gen_df2.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scaler.inverse_transform(testX))\n",
    "print(scaler.inverse_transform([testY]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df.to_csv('gen_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.Open_Interest_All.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras_tuner import Objective\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, start_from_epoch=100, restore_best_weights=True)\n",
    "\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "dataset = dataframe.Target_Open_Interest_All\n",
    "dataset = dataset[1:-1].values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "#look_back = 3\n",
    "LOOK_BACK = 3\n",
    "trainX, trainY = create_dataset(train, LOOK_BACK)\n",
    "testX, testY = create_dataset(test, LOOK_BACK)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Choice('units', [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]), input_shape=(LOOK_BACK, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=hp.Choice('loss', [\"mae\", \"mse\"]), metrics=['mean_absolute_error'], optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "tuner = kt.GridSearch(\n",
    "    hypermodel = build_model,\n",
    "    objective=Objective(\"val_mean_absolute_error\", \n",
    "                        direction=\"min\"),\n",
    "    directory=\"Keras_tuner_dir\",\n",
    "    project_name=\"Keras_tuner_Demo\", \n",
    "    overwrite=True,\n",
    "    seed=42,\n",
    "    max_trials=2500,\n",
    "    \n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    validation_data=(testX, testY),\n",
    "    epochs=100,\n",
    "    #callbacks=[callback],\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "best_hps= tuner.get_best_hyperparameters(1)[0]\n",
    "print(best_hps)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with time step regression framing\n",
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "tf.random.set_seed(7)\n",
    "keras.utils.set_random_seed(42)\n",
    "# load the dataset\n",
    "dataframe = pd.read_csv(\"Temp2.csv\", index_col=0)\n",
    "dataset = dataframe.Open_Interest_All\n",
    "dataset = dataset.values\n",
    "dataset = dataset.astype('float32')\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(look_back, 1), ))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = mean_absolute_error(trainY[0], trainPredict[:,0])\n",
    "print('Train Score: %.2f mean_absolute_error' % (trainScore))\n",
    "testScore = mean_absolute_error(testY[0], testPredict[:,0])\n",
    "print('Test Score: %.2f mean_absolute_error' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset)[-100:])\n",
    "#plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot[-100:])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
